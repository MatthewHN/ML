{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salary Estimator for Tech Positions in the USA\n",
    "### Made by:\n",
    "- Matthew S. Horne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I have chosen a Kaggle Database on the salaries earned by employees in job positions in the USA. I believe that analyzing this data can be interesting, even for personal benefit, as some friends and even myself are considering the possibility of going overseas to work. The data is specifically selected from companies in the Tech sector, although, as we will see, there are positions in Marketing and Human Resources, all of these are from technological companies.**\n",
    "\n",
    "**The Database collects data from a sample of over 6500 employees, for whom the following information is provided:**\n",
    "\n",
    "Age\n",
    "Gender\n",
    "Level of education\n",
    "Job position\n",
    "Years of experience in the position\n",
    "Salary\n",
    "\n",
    "**In this model, we will attempt to estimate an employee's salary based on the rest of the variables.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First econometrics models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T01:00:59.299803Z",
     "start_time": "2025-05-10T01:00:56.773491Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import statsmodels.stats.outliers_influence as oi\n",
    "import statsmodels.graphics.api as smg\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.stats.diagnostic as diag"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, I like to have all the imports together and not have to repeat them or execute specific cells to avoid compiling the entire notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Salary_Data.csv')\n",
    "\n",
    "X = data.values[:, [0, 4]].astype(int)  # Age, Years of Experience\n",
    "Y = data.values[:, 5].astype(int)  # Salary\n",
    "\n",
    "results = sm.OLS(Y, sm.add_constant(X)).fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R-squared:**\n",
    "\n",
    "R-squared is a measure of how well the regression model fits the data. In this case, the value is 0.665, meaning that approximately 66.5% of the variability in the dependent variable (y) can be explained by the independent variables (x1 and x2) in the model. A higher R-squared value indicates a better fit of the model to the data. This value is expected to increase as the model becomes more complex and incorporates more key variables like Job Title or Education.\n",
    "\n",
    "**Adjusted R-squared:**\n",
    "\n",
    "Adjusted R-squared is similar to R-squared but takes into account the number of independent variables in the model. Here, the value is also 0.665, suggesting that the model's fit is consistent with the number of independent variables included.\n",
    "\n",
    "**F-statistic:**\n",
    "\n",
    "The F-statistic is used to evaluate the overall significance of all the independent variables in the model. A large F-statistic value (in this case, 6539) with a small p-value (0.00) suggests that at least one of the independent variables significantly explains variability in the dependent variable.\n",
    "\n",
    "**Coefficients:**\n",
    "\n",
    "Under the \"coef\" section, the estimated coefficients for the variables in the model are observed. In this model, there are three coefficients: one for the constant (intercept), one for x1, and another for x2. These coefficients indicate how much the dependent variable (y) changes per unit change in the independent variables (x1 and x2). For instance, the coefficient for x2 is 9044.7257, implying that, all other variables being constant (ceteris paribus), an increase of one unit in x2 is associated with an approximate increase of 9044.7257 units in the dependent variable (y), meaning that for every additional year of experience (Years of Experience = x2), the salary (Salary = y) increases by 9044.7257$.\n",
    "\n",
    "**Additional Statistics:**\n",
    "\n",
    "Several additional statistics are provided, such as the Omnibus, Durbin-Watson, Jarque-Bera, Skew, and Kurtosis. These statistics help evaluate assumptions about the model and the normality of the residual errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <strong>Note: </strong> Since we are using a database with variables that are of string type, for this first model we will only use numeric variables and which have had to be necessarily converted to int because otherwise, it will generate a type error.\n",
    "    <br><br>\n",
    "    Later on, it will be necessary to convert the categorical variables into dummies for the correct functioning.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Test model (not related to the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "X = np.random.normal(0, 10, n)\n",
    "Y = X + np.random.normal(0, 1, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, Y, s=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sm.OLS(Y, sm.add_constant(X)).fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cte = results.params[0]\n",
    "beta1 = results.params[1]\n",
    "\n",
    "plt.plot([-20, 20], [cte + beta1 * (-20), cte + beta1 * 20], color='r')\n",
    "plt.scatter(X, Y, s=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code blocks perform a simulation of data and fit a linear regression model to such data.\n",
    "\n",
    "1. **Data Generation:** Two datasets, X and Y, are created, where X are values taken from a normal distribution with a mean of 0 and standard deviation of 10, and Y is a linear function of X plus a normally distributed random error term. This simulates a linear relationship between X and Y with some noise.\n",
    "\n",
    "2. **Data Visualization:** A scatter plot is generated using matplotlib to visualize the relationship between X and Y. Each point represents an observation from the simulated dataset.\n",
    "\n",
    "3. **Fitting of Regression Model:** An ordinary least squares linear regression model is fitted using statsmodels with Y as the dependent variable and X as the independent. A constant is added to the model to include an intercept term. A summary of the model is printed, providing statistical details of the fit.\n",
    "\n",
    "4. **Visualization of Regression Model:** The intercept and slope (coefficients) from the fitted model are extracted and used to draw the regression line over the existing scatter plot. The red line represents the estimated relationship between X and Y according to the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation and Inference in Linear Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Salary_Data.csv')\n",
    "\n",
    "# Displays descriptive statistics of the numerical variables\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Count:** Displays the total number of non-null entries for each variable.\n",
    "   - In this case, there are 6582 entries for each of the variables: Age, Years of Experience, and Salary.\n",
    "\n",
    "2. **Mean:** It is the average of the values for each variable.\n",
    "   - The average age is approximately 33.57 years.\n",
    "   - The average years of experience are approximately 8.07 years.\n",
    "   - The average salary is approximately 115,768.67 monetary units ($).\n",
    "\n",
    "3. **Std (Standard Deviation):** Measures the amount of variation or dispersion of a set of values.\n",
    "   - The standard deviation of age is approximately 7.6 years, indicating the variability of age in the dataset.\n",
    "   - The standard deviation of years of experience is approximately 6.04 years.\n",
    "   - The standard deviation of the salary is approximately 52,677.91, indicating the variability in salaries.\n",
    "\n",
    "4. **Min (Minimum):** It is the lowest value in each column.\n",
    "   - The minimum age is 21 years.\n",
    "   - The minimum years of experience is 0 (people with no prior experience).\n",
    "   - The minimum salary is 25,000.\n",
    "\n",
    "5. **25% (25th Percentile):** This is the value below which 25% of the data falls.\n",
    "   - 25% of the employees are 28 years or younger.\n",
    "   - 25% have 3 years or less of experience.\n",
    "   - 25% earn 70,000 or less.\n",
    "\n",
    "6. **50% (Median or 50th Percentile):** It is the median value, where half of the data is below this value and the other half above.\n",
    "   - The median age is 32 years.\n",
    "   - The median years of experience is 7 years.\n",
    "   - The median salary is 115,000.\n",
    "\n",
    "7. **75% (75th Percentile):** The value below which 75% of the data falls.\n",
    "   - 75% of the employees are 38 years or younger.\n",
    "   - 75% have 12 years or less of experience.\n",
    "   - 75% earn 160,000 or less.\n",
    "\n",
    "8. **Max (Maximum):** It is the highest value in each column.\n",
    "   - The maximum age is 62 years.\n",
    "   - The maximum years of experience is 34 years.\n",
    "   - The maximum salary is 250,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Salary_Data.csv')\n",
    "data = pd.get_dummies(data, columns=['Education Level', 'Job Title', 'Gender'], dtype=int)\n",
    "\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <strong>Note:</strong> I have spent hours stuck creating dummies; when creating dummies by default, the types are set to TRUE or FALSE, however, we only work with numbers so, after hours I found out that it only needed to be cast to int type (dtype=int).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Salary']\n",
    "\n",
    "# Filter columns that start with 'Education Level_', 'Job Title_', and 'Gender_*'\n",
    "education_columns = [col for col in data if col.startswith('Education Level_')]\n",
    "job_columns = [col for col in data if col.startswith('Job Title_')]\n",
    "gender_columns = [col for col in data if col.startswith('Gender_')]\n",
    "\n",
    "# Define 'X' including all columns of 'Education Level_', 'Job Title_', and 'Gender_*' + 'Years of Experience' and 'Age'\n",
    "X = sm.add_constant(data[education_columns + ['Years of Experience'] + job_columns + ['Age'] + gender_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(y)\n",
    "Q1 = np.quantile(y, 0.25)\n",
    "Q3 = np.quantile(y, 0.75)\n",
    "StandardDeviation = np.std(y)\n",
    "mean = np.mean(y)\n",
    "histogram = plt.hist(y, bins='auto', rwidth=0.85, density=True)\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(\"Histogram of y (salary) ($)\")\n",
    "plt.show()\n",
    "print(\"Q1: \", Q1, \"($) mean:\", mean, \"($) Q3: \", Q3, \"($) SD: \", StandardDeviation, \"($) Mean:\", np.mean(y), \"($)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Mean: 115,768.67\n",
    "2. First Quartile (Q1): 70,000.00\n",
    "3. Third Quartile (Q3): 160,000.00\n",
    "4. Standard Deviation: 52,673.91\n",
    "5. Median: 115,000.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.asarray(education_columns)\n",
    "ols1 = sm.OLS(y, X).fit()\n",
    "print(ols1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <strong>Note:</strong> As we can see, we are already being warned that there may be multicollinearity problems; we will try to solve it later on when the time comes, perhaps with a manual cleaning of the data or using techniques such as the variance inflation factor (VIF) to identify and possibly eliminate independent variables that are highly correlated.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of the Results\n",
    "\n",
    "#### Goodness of Fit Measures:\n",
    "\n",
    "- **R-squared:**\n",
    "    The value is 0.839, which indicates that approximately 83.9% of the variability in the salary can be explained by the model. This is a quite high measure of goodness of fit considering that we are trying to estimate salaries.\n",
    "\n",
    "- **Adjusted R-squared:**\n",
    "    With a value of 0.837, after adjusting for the number of predictors, it remains very high, indicating that the model fits well to the data.\n",
    "\n",
    "- **F-statistic:**\n",
    "    With a value of 328.4 and a Prob (F-statistic) close to 0, it suggests that the model is statistically significant as a whole, that is, there is evidence that at least one of the independent variables is related to the salary.\n",
    "\n",
    "#### Residual Diagnostics:\n",
    "- **Durbin-Watson:**\n",
    "    The value is 0.216, which is well below 2, suggesting the presence of positive autocorrelation among the residuals, which could be a problem since the residuals of a well-fitted model should be independent of each other.\n",
    "\n",
    "- **Jarque-Bera (JB) and Prob(JB):**\n",
    "    The JB statistic value is 178.435 and the p-value is extremely small, indicating that the residuals do not follow a normal distribution, which is a violation of one of the assumptions of OLS regression.\n",
    "\n",
    "- **Skew:**\n",
    "    The value of -0.062 indicates that the distribution of residuals is slightly asymmetrical, but it is not a major concern given that it is close to zero.\n",
    "\n",
    "- **Kurtosis:**\n",
    "    A value of 3.797 suggests that the distribution of the residuals has heavier tails than a normal distribution, which could be a sign of outliers or a more pronounced peak.\n",
    "\n",
    "- **Cond. No. (Condition Number):**\n",
    "    The value is extremely high (4.75e+16), indicating the presence of multicollinearity among the predictor variables. This means that some of the independent variables are highly correlated with each other, which can inflate the standard errors of the coefficients and make the estimates unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta2 = ols1.params\n",
    "plt.plot(data['Years of Experience'], y, 'o', color='r')\n",
    "xmin = np.min(data['Years of Experience'])\n",
    "xmax = np.max(data['Years of Experience'])\n",
    "\n",
    "plt.plot([xmin, xmax], [beta2.iloc[0] + beta2.iloc[1] * xmin, beta2.iloc[0] + beta2.iloc[1] * xmax])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sm.graphics.plot_regress_exog(ols1, 'Years of Experience'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = ols1.resid\n",
    "print(e)\n",
    "print(np.mean(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtained value suggests that, on average, the model slightly underestimates the actual value. However, in the context of regression models, an average of residuals of about -5.7624e-10 could be considered negligible, especially if the scale of the response variable (salary in this case) is large, as in tens or hundreds of thousands. This result is a sign that the model is well-calibrated in terms of not having a systematic bias towards overestimation or underestimation in the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Total Sum of Squares (SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ols1.centered_tss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It represents the total variability in the dependent variable. It is equal to the sum of the squared differences between each observed value and the mean of all observed values. A value of 18262027724544.66 indicates the total variability in your dependent variable that the model seeks to explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explained Sum of Squares (ESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ols1.ess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represents the variability in the dependent variable that is explained by the model. It is equal to the sum of the squared differences between the predicted values by the model and the mean of the dependent variable. A value of 15326748750986.516 suggests that this is the amount of variability that the model has managed to explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Residual Sum of Squares (RSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ols1.ssr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represents the variability in the dependent variable that is not explained by the model. It is equal to the sum of the squares of the residuals (errors) from the model. A value of 2935278973558.1436 indicates the amount of variability that the model has not been able to explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $R^2$ and Adjusted $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2: \", ols1.rsquared)\n",
    "print(\"R2 Ajustado: \", ols1.rsquared_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared ($R^2$): Is a measure of the goodness of fit of the model. A value of 0.8392687264616815 (or approximately 83.93%) means that about 83.93% of the variability in the dependent variable is explained by the independent variables in the model.\n",
    "\n",
    "Adjusted R-squared: Is an adjusted version of R-squared that takes into account the number of predictors in the model. A value of 0.8367131041747956 is quite close to the R-squared, indicating that the model is robust even after adjusting for the number of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experimental F-value ( F<sub>exp</sub> )\n",
    "- Theorical F-value ( F<sub>teo</sub> )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fexp = ols1.fvalue\n",
    "print(Fexp)\n",
    "from scipy import stats\n",
    "\n",
    "alpha = 0.025\n",
    "Fteo = stats.f.ppf(1 - alpha, ols1.df_model, ols1.df_resid)\n",
    "print(Fteo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental F-value (Fexp): Is the calculated value of the F statistic for the model. A value of 328.40092636864495 is quite high, suggesting that the model is statistically significant.\n",
    "\n",
    "Theoretical F-value (Fteo): Is the critical value of the F-distribution for a certain level of significance and degrees of freedom. A value of 1.2941958816630579 is the threshold above which the model would be considered to have a significant contribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experimental t-value ( t<sub>exp</sub> )\n",
    "- Theorical t-value ( t<sub>teo</sub> )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texp = ols1.tvalues\n",
    "print(texp)\n",
    "alpha = 0.098\n",
    "tteo = stats.t.ppf(1 - (alpha / 2), ols1.df_resid)\n",
    "print(tteo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental t-Value (t-Test Value): This value is the result of a Student's t-test applied to an estimator, such as a coefficient in a regression model. It is calculated as the estimated coefficient divided by the standard error of that coefficient. In the context of regression, it indicates how many standard deviations the estimator is away from 0. A high t-value (in absolute terms) suggests that it is less likely that the true value of the parameter is 0, implying that the corresponding variable is significant in the model.\n",
    "\n",
    "Theoretical t-Value (Critical t-Value): Is used as a threshold to decide whether to reject the null hypothesis. If the experimental t-value is greater in absolute value than the theoretical t-value, the null hypothesis is rejected (for example, that a coefficient is equal to zero in regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Confidence Intervals of Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ols1.conf_int(0.075))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confidence interval provides a range of values within which the true value of the parameter is expected to fall, with a certain level of confidence.\n",
    "In regression, each coefficient has an associated confidence interval. If a confidence interval for a coefficient does not include 0, this suggests that the corresponding variable is significantly different from 0, implying that it has a significant effect on the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Estimation of the disturbance variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta3 = np.array(ols1.params)\n",
    "e = ols1.resid\n",
    "sum(e ** 2) / (ols1.nobs - 1)\n",
    "sigmagorro = (np.dot(y.values, y.values) - np.dot(beta3.T, np.dot(X.values.T, y.values))) / (ols1.nobs - 1)\n",
    "\n",
    "print(sigmagorro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A low value indicates that the model explains a large part of the variability, while a high value suggests that there are more uncaptured factors influencing salaries.\n",
    "In our case, we see that the value is high and that the residuals are not normally distributed, it may be necessary to transform the variables, add terms to the model, or use another type of regression model that does not require normality of the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have previously seen, our current model has a multicollinearity problem that we will try to resolve now. Let's refresh our memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ols1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as we have mentioned, it warns us of possible multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ols1.condition_number)  #Condition Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A high condition number suggests possible multicollinearity in the data, meaning that at least one of the predictor variables is almost a linear combination of the others. This can lead to problems in estimating the coefficients of the model, as small changes in the data or in the model can result in large changes in the estimated coefficients. In practical terms, a high condition number can make the results of the regression unreliable.\n",
    "\n",
    "**In general terms:**\n",
    "\n",
    "- A condition number close to 1 indicates a well-conditioned problem (low risk of multicollinearity).\n",
    "- A moderately high condition number (values in the thousands or tens of thousands) may be cause for some concern and warrants further investigation.\n",
    "- A very high condition number (values in the hundreds of thousands or more) suggests a high degree of multicollinearity and a potentially poorly conditioned problem.\n",
    "\n",
    "In our case, we see that this is an enormous number, the error is possibly due to the creation of the dummies which create 200 different new variables.\n",
    "Therefore, we are going to use techniques such as the variance inflation factor (VIF) to identify and possibly eliminate independent variables that are highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Salary_Data.csv')\n",
    "data = pd.get_dummies(data, columns=['Education Level', 'Job Title', 'Gender'], dtype=int)\n",
    "\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Salary']\n",
    "\n",
    "education_columns = [col for col in data if col.startswith('Education Level_')]\n",
    "job_columns = [col for col in data if col.startswith('Job Title_')]\n",
    "gender_columns = [col for col in data if col.startswith('Gender_')]\n",
    "\n",
    "X = sm.add_constant(data[education_columns + ['Years of Experience'] + job_columns + ['Age'] + gender_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vifs = [oi.variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vifs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the warning \"RuntimeWarning: divide by zero encountered in scalar divide\" which suggests that in the calculation of some VIF, a division by zero is being attempted. This usually occurs when one of the independent variables in the regression model is a perfect linear combination of other independent variables, or it has an extremely low variance that is causing numerical issues.\n",
    "\n",
    "The VIF values are displayed in the output, and many of them are infinite, denoted by inf. An infinite VIF value indicates perfect multicollinearity, meaning that some predictor variables can be exactly predicted from other predictor variables in the model without error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = np.corrcoef(X.T)\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smg.plot_corr(corr_matrix, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite evident what could be the first remedy for this problem... Reducing the number of variables, this will be achieved by studying which variables have no relevance to eliminate them and by grouping those that are positive for the model.\n",
    "\n",
    "Let's do a very simple first test just by adding drop_first=True when creating the dummies to eliminate the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Salary_Data.csv')\n",
    "data = pd.get_dummies(data, columns=['Education Level', 'Job Title', 'Gender'], drop_first=True, dtype=int)\n",
    "y = data['Salary']\n",
    "\n",
    "education_columns = [col for col in data if col.startswith('Education Level_')]\n",
    "job_columns = [col for col in data if col.startswith('Job Title_')]\n",
    "gender_columns = [col for col in data if col.startswith('Gender_')]\n",
    "\n",
    "X = sm.add_constant(data[education_columns + ['Years of Experience'] + job_columns + ['Age'] + gender_columns])\n",
    "\n",
    "ols1 = sm.OLS(y, X).fit()\n",
    "print(ols1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ols1.condition_number)  #Condition Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply with that previous step, we have gone from a Condition Number = (4.75e+16) that our original model had to a Condition Number = (2.94e+03).\n",
    "\n",
    "And we run the VIF technique again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vifs = [oi.variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vifs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, almost all the variables are now close to the value of 1, which as we have seen indicates a well-conditioned problem (low risk of multicollinearity).\n",
    "\n",
    "But we can continue to refine it. If we look closely, there are 2 variables in particular that stand out more than the others, the first with an approximate value of 148.191 and the penultimate with an approximate value of 11.852499.\n",
    "\n",
    "Seeing this and also with a bit of common sense we can reason that when estimating salaries, both the age and the gender of the person should not be a factor influencing the result, so let's eliminate those 2 variables and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Salary_Data.csv')\n",
    "data = pd.get_dummies(data, columns=['Education Level', 'Job Title', 'Gender'], drop_first=True, dtype=int)\n",
    "\n",
    "#data.drop('Job Title_Data Analyst', axis=1, inplace=True)\n",
    "#data.drop('Job Title_Marketing Manager', axis=1, inplace=True)\n",
    "#data.drop('Job Title_Web Developer', axis=1, inplace=True)\n",
    "\n",
    "y = data['Salary']\n",
    "\n",
    "education_columns = [col for col in data if col.startswith('Education Level_')]\n",
    "job_columns = [col for col in data if col.startswith('Job Title_')]\n",
    "gender_columns = [col for col in data if col.startswith('Gender_')]\n",
    "\n",
    "X = sm.add_constant(data[education_columns + ['Years of Experience'] + job_columns])\n",
    "#X = sm.add_constant(data[education_columns + job_columns])\n",
    "#X = sm.add_constant(data[education_columns + ['Years of Experience']])\n",
    "\n",
    "ols1 = sm.OLS(y, X).fit()\n",
    "print(ols1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vifs = [oi.variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ols1.condition_number)  #Condition Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, by eliminating variables that do not have a direct relationship with the salary, the Condition Number = (2.94e+03) that our model had with Age and Gender has been reduced to just a Condition Number = 834 without sacrificing the R-squared measure.\n",
    "\n",
    "We are no longer warned that there may be problems with multicollinearity, but let's try to reduce the number of variables by grouping elements of the Job Title variable by sectors, this can lead to more imprecise estimates by having more general data from each area but let's try to see what Condition Number would result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <strong>Note:<br><br></strong> You can try by removing the '#' that I have placed in the cell above.\n",
    "    <br><br>\n",
    "    Eliminating the 'Years of Experience' variable, the Cond. No. goes from 833.5492 to 89.2, but R-squared is affected by a decrease from 0.838 to 0.680.\n",
    "    <br><br>\n",
    "    Eliminating the 'job_columns' variable, the Cond. No. goes from 833.5492 to 43.8, but R-squared is affected by a decrease from 0.838 to 0.709.\n",
    "    <br><br>\n",
    "    The data.drop commands are not necessary but are there because they can reduce the value of the VIF indices. They simply remove more columns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For this next test, I created a duplicate of the database, but I have significantly reduced the number of variables, especially from the Job Titles column in which I have grouped similar Positions into one.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Salary_Data - Copy.csv')\n",
    "data = pd.get_dummies(data, columns=['Education Level', 'Job Title'], drop_first=True, dtype=int)\n",
    "\n",
    "y = data['Salary']\n",
    "\n",
    "education_columns = [col for col in data if col.startswith('Education Level_')]\n",
    "job_columns = [col for col in data if col.startswith('Job Title_')]\n",
    "\n",
    "X = sm.add_constant(data[education_columns + ['Years of Experience'] + job_columns])\n",
    "\n",
    "ols2 = sm.OLS(y, X).fit()\n",
    "print(ols2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vifs = [oi.variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = np.corrcoef(X.T)\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smg.plot_corr(corr_matrix, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we see, this reduction/grouping of data has again reduced the Cond. No. from 834 to 213.**\n",
    "\n",
    "**Moreover, all this has been possible without sacrificing the goodness of fit, which continues to be quite good, thus solving the problem of multicollinearity.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heteroscedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Previously, we have solved multicollinearity, now we are going to check if our model follows a normal distribution of residuals (homoscedasticity) or not (heteroscedasticity).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ols1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ['Jarque-Bera Est', 'Jarque-Vera p-val', 'Skew', 'Kurtosis']\n",
    "test = sms.jarque_bera(ols1.resid)\n",
    "for i in range(4):\n",
    "    print(name[i], test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Jarque-Bera statistic is 187.68, which is very high. This test combines skewness and kurtosis to assess whether the distribution of residuals deviates from normality. A higher value indicates a greater deviation from normality.\n",
    "\n",
    "- The p-value associated with the Jarque-Bera statistic is extremely small (around 1.76e-41). Such a low p-value indicates that we can reject the null hypothesis that the residuals have a normal distribution.\n",
    "\n",
    "- The skewness of the residuals is -0.1125, indicating a slight leftward skew. However, this value is quite close to zero, suggesting that the skewness is not very pronounced.\n",
    "\n",
    "- The kurtosis is 3.7960, which is slightly higher than the value of 3 expected from a normal distribution. This indicates that the distribution of residuals has slightly heavier tails than a normal distribution, although this value is not too far from 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "pyplot.hist(ols1.resid)\n",
    "pyplot.show()\n",
    "qqplot(ols1.resid, line='s')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diag.kstest_normal(ols1.resid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first value (0.0676) is the Kolmogorov-Smirnov D statistic, which measures the maximum distance between the cumulative distribution function (CDF) of the residuals and the CDF of a normal distribution. A higher value indicates a greater deviation from normality.\n",
    "\n",
    "- The second value (approximately 0.0) is the p-value of the test. Since it is lower than any commonly used threshold for statistical significance (such as 0.05 or 0.01), the null hypothesis that the distribution of residuals is normal is rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Salary_Data.csv')\n",
    "data = pd.get_dummies(data, columns=['Education Level', 'Job Title', 'Gender'], drop_first=True, dtype=int)\n",
    "\n",
    "y = data['Salary']\n",
    "\n",
    "# Define 'X' including all columns of 'Education Level_*' and 'Years of Experience'\n",
    "# Filter columns that start with 'Education Level_'\n",
    "education_columns = [col for col in data if col.startswith('Education Level_')]\n",
    "job_columns = [col for col in data if col.startswith('Job Title_')]\n",
    "\n",
    "X = sm.add_constant(data[education_columns + ['Years of Experience'] + job_columns])\n",
    "\n",
    "from random import choices\n",
    "\n",
    "beta = []\n",
    "n = len(y)\n",
    "for it in range(100):  #we repeat the estimation 100 times\n",
    "    I = choices(list(range(n)), k=n)  # we choose a sample with repetition of the data\n",
    "    ols3 = sm.OLS(y[I], sm.add_constant(X.values[I, :])).fit()  # we fit the model\n",
    "    beta.append(list(ols3.params))  # we save the coefficients\n",
    "beta = np.array(beta)\n",
    "k = len(X.T)\n",
    "for i in range(k):\n",
    "    q025 = np.percentile(beta[:, i], 2.5)  #2.5th percentile\n",
    "    q975 = np.percentile(beta[:, i], 97.5)  #97.5th percentile\n",
    "    mean = np.mean(beta[:, i])  #mean of the betas\n",
    "    sd = np.std(beta[:, i])  #standard deviation of the betas\n",
    "    print(i, mean, [q025, q975])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initializes an empty beta list to store the results of the coefficients from the bootstrap simulations. Perform 100 bootstrap iterations. In each iteration:\n",
    "- Select a random sample with replacement from the indices of the data (this simulates resampling of the data).\n",
    "- Fit a linear regression model to the resampled sample.\n",
    "- Save the estimated coefficients of the model in the beta list.\n",
    "\n",
    "The calculated percentiles represent the 95% confidence interval limits for the coefficients, based on the bootstrap distribution. The purpose of this process is to estimate the distribution of the regression model coefficients beyond what the model fitted to the original data can provide. Bootstrapping is especially useful when the distribution of the estimators is not known or is difficult to derive analytically. The confidence intervals calculated from the bootstrap distribution can provide a robust measure of the uncertainty in the estimations of the coefficients."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ols1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOLDFELD-QUANDT (Muestras Pequeñas)\n",
    "GQ = sms.het_goldfeldquandt(y, sm.add_constant(data[\"Years of Experience\"]), split=1 / 3, drop=1 / 3)\n",
    "print(\"Goldfeld Quandt: \", GQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 0.910837699262646 is the test statistic for the Goldfeld-Quandt.\n",
    "- 0.9855773263763483 is the p-value of the test.\n",
    "\n",
    "Given that the p-value is high, there is not enough evidence to reject the null hypothesis of homoscedasticity, which means that there is no evidence of heteroscedasticity in the data according to this test; therefore, we will continue testing with other variables and methods."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOLDFELD-QUANDT (Muestras Pequeñas)\n",
    "GQ = sms.het_goldfeldquandt(y, sm.add_constant(data[\"Job Title_Data Scientist\"]), split=1 / 3, drop=1 / 3)\n",
    "print(\"Goldfeld Quandt: \", GQ)\n",
    "GQ = sms.het_goldfeldquandt(y, sm.add_constant(data[\"Job Title_Software Engineer\"]), split=1 / 3, drop=1 / 3)\n",
    "print(\"Goldfeld Quandt: \", GQ)\n",
    "GQ = sms.het_goldfeldquandt(y, sm.add_constant(data[\"Job Title_Product Manager\"]), split=1 / 3, drop=1 / 3)\n",
    "print(\"Goldfeld Quandt: \", GQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOLDFELD-QUANDT (Muestras Pequeñas)\n",
    "GQ = sms.het_goldfeldquandt(y, sm.add_constant(data[\"Education Level_PhD\"]), split=1 / 3, drop=1 / 3)\n",
    "print(\"Goldfeld Quandt PhD: \", GQ)\n",
    "GQ = sms.het_goldfeldquandt(y, sm.add_constant(data[\"Education Level_Master\\'s Degree\"]), split=1 / 3, drop=1 / 3)\n",
    "print(\"Goldfeld Quandt Master: \", GQ)\n",
    "GQ = sms.het_goldfeldquandt(y, sm.add_constant(data[\"Education Level_High School\"]), split=1 / 3, drop=1 / 3)\n",
    "print(\"Goldfeld Quandt HS: \", GQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "With a few more tests, we observed how the p-value for 'Education Level_High School' is minuscule. A p-value below a common significance level (like 0.05 or 0.01) indicates that there is sufficient evidence to reject the null hypothesis that the variance of the residuals is constant throughout the range of the data, that is, that there is heteroscedasticity. However, to confirm the hypothesis, we corroborate it with other techniques for detecting heteroscedasticity."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BREUSH-PAGAN\n",
    "BP = sms.het_breuschpagan(ols1.resid, ols1.model.exog)\n",
    "print(\"Breush Pagan: \", BP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHITE\n",
    "W = sms.het_white(ols1.resid, ols1.model.exog)\n",
    "print(\"White: \", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In both cases, the reported p-values are 0.0, indicating that there is significant statistical evidence to reject the null hypothesis that the errors are homoscedastic (have constant variances). This means that the results of both tests point to the presence of heteroscedasticity in the residuals of the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ols1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLEJSER\n",
    "for h in [-2, -1, -0.5, 0.5, 1, 2]:\n",
    "    # Make sure there are no zeros in the data before raising to a negative power\n",
    "    X_nonzero = X.replace(0, np.nan)  # Replace 0 with NaN to avoid divisions by zero\n",
    "    X_transformed = X_nonzero.astype(float) ** h  # Raise to the power of h\n",
    "    X_transformed = X_transformed.fillna(0)  # Replace NaNs with 0s if necessary, depending on the context\n",
    "    X_transformed = X_transformed.replace([np.inf, -np.inf], np.nan)  # Replace inf with NaN\n",
    "    X_transformed = X_transformed.dropna(axis=1, how='all')  # Remove columns where all values are NaN\n",
    "\n",
    "    olsaux = sm.OLS(abs(ols1.resid), sm.add_constant(X_transformed)).fit()\n",
    "    beta3 = olsaux.params\n",
    "    pval = beta3.iloc[1]\n",
    "    print(\"h: \", h, \"-> pval:\", pval, \"R2: \", olsaux.rsquared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <strong>Note:</strong> The Glejser test is a method used to detect heteroscedasticity in the errors of a regression model. It specifically looks for relationships between the absolute values of the residuals of a regression model and the independent variables or transformations of these variables. If there is a significant relationship, it suggests that the variance of the errors changes with the level of the independent variable, which is a form of heteroscedasticity.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For h= −2, the p-value is very small, suggesting that there is a significant relationship between the residuals and the independent variables raised to the power of -2, indicating heteroscedasticity.\n",
    "This pattern repeats for other values of h, with very small p-values, indicating that heteroscedasticity is consistently detected for various transformations of the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp = sm.WLS(y, sm.add_constant(X), weights=1 / y).fit()\n",
    "print(mcp.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since with ols1 we are not achieving the desired results in heteroscedasticity, nor by applying other regression models such as weighted least squares (WLS), let's try with a variant of ordinary least squares regression which we will call ols2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's refresh our memory about ols2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Salary_Data - Copy.csv')\n",
    "data = pd.get_dummies(data, columns=['Education Level', 'Job Title'], drop_first=True, dtype=int)\n",
    "y = data['Salary']\n",
    "\n",
    "# Define 'X' including all columns of 'Education Level_*' and 'Years of Experience'\n",
    "# Filter columns that start with 'Education Level_'\n",
    "education_columns = [col for col in data if col.startswith('Education Level_')]\n",
    "job_columns = [col for col in data if col.startswith('Job Title_')]\n",
    "\n",
    "X = sm.add_constant(data[['Years of Experience'] + job_columns + education_columns])\n",
    "\n",
    "ols2 = sm.OLS(y, X).fit()\n",
    "print(ols2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BREUSH-PAGAN\n",
    "BP = sms.het_breuschpagan(ols2.resid, ols2.model.exog)\n",
    "print(\"Breush Pagan: \", BP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHITE\n",
    "W = sms.het_white(ols2.resid, ols2.model.exog)\n",
    "print(\"White: \", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "dw = durbin_watson(ols2.resid)\n",
    "print(dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Although there is an improvement in ols2 compared to ols1, we see that the tests still indicate the presence of heteroscedasticity. Additionally, we observe in the Durbin-Watson factor that we have autocorrelation problems as it is far from 2**\n",
    "\n",
    "**In summary: we still have problems with heteroscedasticity and autocorrelation. These will be addressed next.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data for ols2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T01:01:16.083117Z",
     "start_time": "2025-05-10T01:01:15.087858Z"
    }
   },
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "data = pd.read_csv('Salary_Data - Copy2.csv')\n",
    "data = pd.get_dummies(data, columns=['Education Level', 'Job Title'], drop_first=True, dtype=int)\n",
    "\n",
    "#y, lambda_opt = boxcox((data['Salary']))\n",
    "y = np.sqrt(data['Salary'])\n",
    "#y = np.log(data['Salary'])\n",
    "\n",
    "job_columns = [col for col in data if col.startswith('Job Title_')]\n",
    "\n",
    "X = sm.add_constant(data[['Years of Experience'] + job_columns])\n",
    "\n",
    "ols2 = sm.OLS(y, X).fit()\n",
    "print(ols2.summary())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 Salary   R-squared:                       0.752\n",
      "Model:                            OLS   Adj. R-squared:                  0.751\n",
      "Method:                 Least Squares   F-statistic:                     1422.\n",
      "Date:                Sat, 10 May 2025   Prob (F-statistic):               0.00\n",
      "Time:                        03:01:16   Log-Likelihood:                -21851.\n",
      "No. Observations:                4241   AIC:                         4.372e+04\n",
      "Df Residuals:                    4231   BIC:                         4.379e+04\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "const                 221.7413      3.874     57.238      0.000     214.146     229.336\n",
      "Years of Experience     8.5990      0.118     72.706      0.000       8.367       8.831\n",
      "Job Title_Analyst      75.3696      4.462     16.890      0.000      66.621      84.118\n",
      "Job Title_Developer    15.9190      4.186      3.803      0.000       7.712      24.126\n",
      "Job Title_Engineer     56.1948      4.102     13.698      0.000      48.152      64.237\n",
      "Job Title_Financial    48.6137      5.580      8.711      0.000      37.673      59.554\n",
      "Job Title_HR           -0.6715      4.532     -0.148      0.882      -9.557       8.214\n",
      "Job Title_Product      66.9974      4.550     14.725      0.000      58.077      75.917\n",
      "Job Title_Sales       -28.4898      4.451     -6.401      0.000     -37.216     -19.764\n",
      "Job Title_Scientist    83.0313      4.368     19.010      0.000      74.468      91.595\n",
      "==============================================================================\n",
      "Omnibus:                        1.722   Durbin-Watson:                   1.995\n",
      "Prob(Omnibus):                  0.423   Jarque-Bera (JB):                1.674\n",
      "Skew:                          -0.031   Prob(JB):                        0.433\n",
      "Kurtosis:                       3.075   Cond. No.                         188.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "Test MSE: 1775.0968955821456\n",
      "Test R²: 0.7388337623191652\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Re-analyzing the model summary we can finally see that we have managed to reduce the Jarque-Bera to low values, this has been achieved by eliminating more variables that did not contribute to the model such as 'Education Level' and using transformations to the dependent variable. Based on testing, we have found that the optimal transformation in our case is the square root, we left commented to test the transformation by logarithms and boxcox. We also highlight that we created a new copy of the data called 'Salary_Data - Copy2.csv' in which we even more randomized the data, grouping each position by its department.\n",
    "\n",
    "   - Skew: A value of 0.010 indicates that the distribution of the residuals is very symmetrical.\n",
    "   - Kurtosis: A value of 3.094 is very close to 3, which is the kurtosis of a normal distribution, indicating a form of distribution of reasonably normal residuals.\n",
    "   - Jarque-Bera (JB): A low value of 2.045 and a p-value of 0.360 suggest there is not enough evidence to reject the hypothesis of normality of the residuals, thus we have solved the problem of heteroscedasticity.\n",
    "\n",
    "In summary, the residuals of the model seem to be distributed in a relatively normal manner based on the Jarque-Bera test, thus concluding our optimization of the model in terms of heteroscedasticity."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = np.corrcoef(X.T)\n",
    "smg.plot_corr(corr_matrix, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "dw = durbin_watson(ols2.resid)\n",
    "print(dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we see how the value of Durbin-Watson has become approximately 2.0237643721784795 compared to its previous version which was approximately 0.2059353521688824 and as we already know a value close to 2 suggests that there is no autocorrelation.\n",
    "\n",
    "This problem has been solved simply by randomizing the data of the database, since, when manipulating them previously we had ordered them and they were serialized around one of its variables. To maintain the previous process and view the evolution of the model, we created a new copy of the data called 'Salary_Data - Copy2.csv' in which we also further group the data, grouping each position by department."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# New DataFrame for the input data\n",
    "input_data = pd.DataFrame({\n",
    "    'const': 1,\n",
    "    'Years of Experience': [10],  # The number of years of experience\n",
    "    'Job Title_Analyst': [0],     # and then a series of flags for job title, all set to 0...\n",
    "    'Job Title_Developer': [0],   # ...except for the one corresponding to the job title of interest.\n",
    "    'Job Title_Engineer': [1],\n",
    "    'Job Title_Financial': [0],\n",
    "    'Job Title_HR': [0],\n",
    "    'Job Title_Product': [0],\n",
    "    'Job Title_Sales': [0],\n",
    "    'Job Title_Scientist': [0]\n",
    "})\n",
    "\n",
    "# Predict the salary using the fitted model\n",
    "sqrt_predicted_salary = ols2.predict(input_data)\n",
    "# Revert the square root transformation by squaring the predicted salary\n",
    "predicted_salary = np.power(sqrt_predicted_salary, 2)\n",
    "\n",
    "print(f\"The predicted salary is: {predicted_salary:}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-10T01:04:07.094372Z",
     "start_time": "2025-05-10T01:04:07.062396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted salary is: 0    102996.934977\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Final Conclusions:\n",
    "\n",
    "1. Significance of Predictors:\n",
    "   - Years of experience and specific job positions have proven to be significant predictors of salaries, which indicates that both factors are important in determining an employee's compensation.\n",
    "   - The regression coefficients for the job positions reflect the salary differences between professions, adjusted for work experience.\n",
    "\n",
    "2. Model Adjustment:\n",
    "   - The model shows a good overall fit, with an adjusted R-squared indicating that a substantial proportion of the variability in salaries can be explained by the variables included in the model.\n",
    "   - The diagnostics of the residuals suggest that the assumption of normality is reasonably well maintained and although there were indications of autocorrelation and heteroscedasticity, they were corrected.\n",
    "\n",
    "3. Multicollinearity Diagnosis:\n",
    "   - An extremely high multicollinearity was discovered due to the large number of initial variables in the data (+200).\n",
    "   - Two clean-ups on the data were carried out that drastically reduced the condition number and significantly reduced multicollinearity.\n",
    "\n",
    "4. Heteroscedasticity Diagnosis:\n",
    "   - Heteroscedasticity was detected in the residuals, which led to the application of transformations and the use of robust standard errors to correct for heteroscedasticity.\n",
    "   - Transformations such as the square root, logarithmic, and Box-Cox were applied to stabilize the variance of the residuals and correct for heteroscedasticity, concluding that the square root was the most optimal.\n",
    "\n",
    "5. Autocorrelation Diagnosis:\n",
    "   - Signs of autocorrelation were found in the residuals, which resulted in an error in the data that was serialized around a variable.\n",
    "   - To solve this, it was only necessary to randomize the data of the database, since they had been previously ordered and serialized around one of its variables. To maintain the previous process and see the evolution of the model we have created a separate database from the one used above and we have called it 'Salary_Data - Copy2.csv'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
